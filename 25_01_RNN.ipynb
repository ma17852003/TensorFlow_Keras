{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然語言處理(NLP)實作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 程式參考來源：\n",
    "- https://keras.io/api/layers/core_layers/embedding/\n",
    "- https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "- https://keras.io/guides/working_with_rnns/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000019DACEF2040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "(32, 10, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.58804767e-02,  1.71773918e-02, -1.62376054e-02,\n",
       "        -2.38022935e-02,  3.62489112e-02,  2.38510408e-02,\n",
       "         4.03696336e-02,  6.44271448e-03,  3.51262204e-02,\n",
       "        -2.70358566e-02,  4.50535864e-03,  4.37803976e-02,\n",
       "        -4.43189740e-02,  2.06765272e-02, -1.67950280e-02,\n",
       "         4.75644954e-02,  2.89392732e-02,  8.01891088e-03,\n",
       "        -1.17008574e-02,  2.79867686e-02,  7.28737563e-04,\n",
       "         4.34629992e-03,  3.50525863e-02,  3.18518169e-02,\n",
       "        -3.75759602e-03, -2.35618278e-03,  4.21545841e-02,\n",
       "        -1.17904320e-02, -1.81266889e-02, -2.18390301e-03,\n",
       "         2.88045295e-02,  2.50958316e-02,  3.09901945e-02,\n",
       "        -2.30692513e-02, -2.09498405e-02,  3.55203412e-02,\n",
       "         1.25880949e-02, -3.52970958e-02, -4.21873108e-02,\n",
       "        -1.78522244e-02, -3.74946371e-02, -2.22765934e-02,\n",
       "        -4.33247574e-02,  2.64650844e-02, -1.74780004e-02,\n",
       "        -7.14511797e-03, -4.38716263e-03, -3.69833484e-02,\n",
       "        -3.89942639e-02,  5.34845516e-03,  2.33540200e-02,\n",
       "         1.56632550e-02,  2.71546282e-02, -4.03502099e-02,\n",
       "        -3.06257010e-02,  3.43697779e-02,  4.49758805e-02,\n",
       "        -3.42451558e-02,  1.45167820e-02,  2.75851376e-02,\n",
       "        -1.86114423e-02,  3.20574380e-02, -4.95682843e-02,\n",
       "        -2.21957918e-02],\n",
       "       [-3.01759597e-02, -4.26419377e-02,  2.91296951e-02,\n",
       "        -4.78874519e-03, -5.68395853e-03, -4.49793413e-03,\n",
       "        -3.19067016e-02,  7.37531111e-03,  2.13879012e-02,\n",
       "        -1.97604429e-02, -4.35156822e-02,  2.01564096e-02,\n",
       "        -4.64164875e-02, -4.63009365e-02,  3.60104926e-02,\n",
       "        -4.58404198e-02, -3.79081592e-02,  4.98248450e-02,\n",
       "        -3.52763534e-02,  9.56367701e-04,  2.72160769e-03,\n",
       "        -4.18247692e-02,  3.22705396e-02,  2.74236463e-02,\n",
       "         3.41305993e-02, -4.99251969e-02, -3.13156731e-02,\n",
       "         2.71573663e-05,  2.35340931e-02, -2.62061488e-02,\n",
       "         1.16265789e-02, -3.77230272e-02, -1.51401535e-02,\n",
       "        -3.63846533e-02, -4.33604717e-02,  1.21960640e-02,\n",
       "        -3.00224777e-02,  2.72704996e-02,  2.51107104e-02,\n",
       "        -4.64598909e-02, -4.93533723e-02, -2.72018667e-02,\n",
       "         2.92607583e-02,  4.82443683e-02,  3.36407758e-02,\n",
       "         1.81101449e-02, -7.68957287e-03,  8.97992775e-03,\n",
       "        -4.41170111e-02,  3.40047143e-02,  2.17720903e-02,\n",
       "         1.66750811e-02, -4.34333570e-02, -2.05749162e-02,\n",
       "        -1.99925900e-02,  5.41503355e-03,  2.63329633e-02,\n",
       "         3.80010344e-02,  1.82068087e-02, -2.06174850e-02,\n",
       "        -1.75212733e-02,  3.20489667e-02, -1.23168230e-02,\n",
       "         2.66871937e-02],\n",
       "       [-1.94245577e-02, -4.75014448e-02, -8.30322504e-03,\n",
       "        -1.43544786e-02, -5.84244728e-04, -4.38302271e-02,\n",
       "        -1.08019710e-02, -1.51236281e-02, -2.18092687e-02,\n",
       "         3.48020904e-02, -1.91156939e-03, -1.37657896e-02,\n",
       "         1.88551098e-03, -6.30768389e-03, -4.32702899e-02,\n",
       "        -2.85074860e-03,  1.77578069e-02,  1.81677379e-02,\n",
       "         2.66636722e-02, -2.23110672e-02,  3.32687609e-02,\n",
       "        -2.75956038e-02,  7.85108656e-03, -2.83652190e-02,\n",
       "         4.07198332e-02,  1.54210664e-02,  7.65883923e-03,\n",
       "        -2.74251457e-02,  3.43380906e-02, -2.97188889e-02,\n",
       "        -7.60308653e-03,  2.97372416e-03, -2.69639976e-02,\n",
       "         3.53093259e-02,  3.32924016e-02, -4.08914089e-02,\n",
       "         4.46215533e-02,  1.32979080e-03, -3.19905505e-02,\n",
       "        -4.60774302e-02, -2.28260756e-02, -3.39065678e-02,\n",
       "        -4.37417142e-02, -2.81076785e-02,  4.84863780e-02,\n",
       "         4.98433597e-02,  1.53265856e-02, -2.22482532e-03,\n",
       "        -3.98557670e-02,  4.39828746e-02, -4.02703285e-02,\n",
       "         2.17681192e-02, -2.69156583e-02,  3.26574780e-02,\n",
       "         1.71491168e-02, -3.28347832e-02, -4.03581932e-03,\n",
       "         2.20453478e-02,  3.13160308e-02,  1.19476914e-02,\n",
       "         9.02377069e-04, -3.07960045e-02,  2.72416137e-02,\n",
       "        -1.10579655e-03],\n",
       "       [-2.08311919e-02,  2.77587213e-02, -3.61225493e-02,\n",
       "        -5.99129125e-03, -1.41528845e-02,  1.88556425e-02,\n",
       "        -1.79449469e-03, -2.11236831e-02,  2.93926857e-02,\n",
       "         2.69946940e-02, -2.60628220e-02,  4.87385280e-02,\n",
       "        -6.89644739e-03, -4.02121916e-02, -1.60948038e-02,\n",
       "        -2.04139352e-02, -2.88902652e-02,  2.99353711e-02,\n",
       "         4.61019613e-02, -1.26677752e-03, -3.84767279e-02,\n",
       "        -5.93669340e-03,  4.62635644e-02, -8.68011266e-04,\n",
       "        -1.68673173e-02,  4.94404919e-02, -3.12741287e-02,\n",
       "         4.85563166e-02,  3.67819555e-02, -5.74065372e-03,\n",
       "        -1.64806470e-02, -3.05979252e-02, -2.06490606e-03,\n",
       "         2.24695355e-03, -3.75509039e-02, -4.52933572e-02,\n",
       "         5.22278249e-04,  1.79946423e-04,  4.79153730e-02,\n",
       "         3.72728966e-02, -4.17835638e-03, -5.92097640e-05,\n",
       "        -2.75344495e-02,  3.74329723e-02, -3.46782431e-02,\n",
       "         2.51532681e-02,  4.51549776e-02,  2.27817632e-02,\n",
       "         3.49203609e-02, -1.65691003e-02, -6.86150789e-03,\n",
       "         2.11416520e-02,  3.71961854e-02, -4.37741280e-02,\n",
       "        -9.11634043e-03,  2.65275501e-02, -6.01398945e-03,\n",
       "         2.12524086e-03, -6.90650940e-03, -2.60983780e-03,\n",
       "        -3.70738879e-02, -1.36023052e-02, -7.59869814e-03,\n",
       "        -1.15542524e-02],\n",
       "       [-4.88665812e-02, -3.67130749e-02,  4.52178828e-02,\n",
       "         4.41826507e-03, -3.15942913e-02, -2.64065340e-03,\n",
       "        -3.17418948e-02, -9.92410257e-03, -4.67450134e-02,\n",
       "         2.84437090e-03,  4.32300903e-02, -3.45468521e-03,\n",
       "         1.04386322e-02, -1.89896468e-02,  4.13041227e-02,\n",
       "        -4.92453575e-02, -2.70091370e-03, -3.68938670e-02,\n",
       "         7.03295320e-03, -5.81999868e-03, -4.61351648e-02,\n",
       "        -4.61228266e-02,  1.23014227e-02, -2.41293795e-02,\n",
       "         1.23893730e-02, -2.27371939e-02, -2.91704666e-02,\n",
       "        -2.92322170e-02,  1.94472075e-03, -1.07401609e-02,\n",
       "         4.05203439e-02, -4.81763370e-02, -1.43792853e-02,\n",
       "        -4.02376764e-02,  4.06051613e-02, -4.59604971e-02,\n",
       "         3.49211209e-02, -3.19248810e-02,  3.39624621e-02,\n",
       "        -1.01967938e-02,  1.62727758e-03,  2.92199738e-02,\n",
       "        -4.15475853e-02, -4.53411825e-02,  4.59575392e-02,\n",
       "        -1.17999539e-02,  4.25785221e-02,  3.38224806e-02,\n",
       "        -1.03638768e-02, -3.29036936e-02, -4.13608328e-02,\n",
       "         1.55526288e-02,  4.52842973e-02,  3.45108658e-03,\n",
       "         3.71857174e-02,  3.89442705e-02,  3.03751566e-02,\n",
       "         2.76724249e-03,  1.92063339e-02, -1.00138299e-02,\n",
       "         3.95313650e-03,  4.06050943e-02, -2.39379536e-02,\n",
       "        -2.46482380e-02],\n",
       "       [-4.77354899e-02,  1.63148306e-02, -4.54832241e-03,\n",
       "         2.99756639e-02,  2.43691914e-02, -1.29910707e-02,\n",
       "         2.94010378e-02,  3.32618989e-02,  3.33755128e-02,\n",
       "         3.04347984e-02,  7.27629662e-03,  2.05421187e-02,\n",
       "         2.44633816e-02, -3.90339009e-02, -1.92944165e-02,\n",
       "         2.16541328e-02, -4.80825081e-02, -2.41398457e-02,\n",
       "        -2.59322766e-02,  3.81655805e-02, -3.68088372e-02,\n",
       "        -1.79640278e-02, -3.50361839e-02,  1.81108378e-02,\n",
       "        -1.29874945e-02,  2.32246183e-02,  2.62170173e-02,\n",
       "         3.70599888e-02,  2.15610154e-02, -1.62156820e-02,\n",
       "        -3.56031656e-02, -2.35968716e-02, -4.02461998e-02,\n",
       "         1.85581334e-02,  1.95373185e-02, -3.90445106e-02,\n",
       "         3.03480513e-02,  4.81347330e-02,  2.30688192e-02,\n",
       "        -1.63801536e-02, -2.96326522e-02, -1.62905678e-02,\n",
       "        -4.64853756e-02,  8.44180584e-03,  4.22981717e-02,\n",
       "         4.73490097e-02,  4.22664322e-02, -4.38819081e-03,\n",
       "        -4.60690632e-02,  6.33116812e-03,  3.62266563e-02,\n",
       "         2.57783867e-02, -4.64948267e-03,  1.60751380e-02,\n",
       "        -3.09463497e-02, -2.11268552e-02, -3.31933051e-02,\n",
       "         1.32972933e-02, -3.50546837e-03,  4.58254553e-02,\n",
       "         4.22774665e-02, -1.89702753e-02, -3.42478976e-02,\n",
       "        -4.11272049e-06],\n",
       "       [-4.37897444e-03,  1.79246776e-02, -1.61420591e-02,\n",
       "         3.69668938e-02, -1.11332908e-02,  2.73825973e-03,\n",
       "         7.65738636e-03, -2.74139885e-02, -4.39789146e-03,\n",
       "        -4.66046110e-02,  1.69601105e-02,  4.51006405e-02,\n",
       "        -1.20700374e-02, -2.01987103e-03,  4.16612662e-02,\n",
       "        -2.74254438e-02, -4.72147390e-03,  2.40146555e-02,\n",
       "        -4.81178612e-03,  4.41014431e-02,  4.13651578e-02,\n",
       "        -3.74520048e-02, -2.50744354e-02, -1.91031229e-02,\n",
       "         3.21552269e-02,  4.95730303e-02, -3.11748870e-02,\n",
       "        -1.20044947e-02,  4.81461026e-02, -9.39445570e-03,\n",
       "         4.69959266e-02,  3.64949591e-02,  3.57291847e-03,\n",
       "         2.56668366e-02,  1.17048621e-02,  3.10968049e-02,\n",
       "        -4.53589223e-02, -3.73995304e-02,  4.92324717e-02,\n",
       "        -2.65060198e-02,  1.18296258e-02, -2.49473695e-02,\n",
       "         1.10533014e-02,  2.71772482e-02, -2.60236617e-02,\n",
       "        -9.24064964e-03,  1.22688897e-02, -4.58683260e-02,\n",
       "         3.39001752e-02, -3.56150977e-02, -3.63233089e-02,\n",
       "         1.38973854e-02,  3.64723913e-02,  1.53673925e-02,\n",
       "         3.28618996e-02,  1.97016634e-02,  2.52035595e-02,\n",
       "        -3.20806354e-03,  1.71456225e-02,  1.07553974e-02,\n",
       "        -3.90288606e-02, -1.56326070e-02,  2.71010064e-02,\n",
       "        -4.41604033e-02],\n",
       "       [ 4.91576269e-03,  3.80056538e-02, -4.72369455e-02,\n",
       "         2.87208818e-02, -7.71458074e-03, -4.71175835e-03,\n",
       "        -1.56314746e-02, -2.22113486e-02,  3.20114940e-03,\n",
       "         3.64831835e-03,  1.27247833e-02, -1.57193318e-02,\n",
       "         3.10857929e-02, -4.46652435e-02,  2.57241465e-02,\n",
       "        -3.13623324e-02, -9.10178572e-03, -9.66625288e-03,\n",
       "        -3.19083929e-02,  4.81170751e-02,  2.36505754e-02,\n",
       "        -4.36164029e-02,  2.28426941e-02,  3.80630754e-02,\n",
       "        -1.60919540e-02,  2.15416886e-02, -4.46086638e-02,\n",
       "         1.04225762e-02,  3.47724594e-02, -4.28026430e-02,\n",
       "        -3.28292474e-02, -4.79240790e-02, -1.96963195e-02,\n",
       "         9.81409475e-03,  4.00137901e-03,  2.20611133e-02,\n",
       "        -2.04375032e-02,  3.69579457e-02, -3.40287313e-02,\n",
       "         2.17834003e-02,  3.04980166e-02, -4.14578989e-03,\n",
       "         1.17628649e-03, -2.75084730e-02, -3.51378545e-02,\n",
       "        -3.66751775e-02,  3.81294154e-02, -3.43347713e-03,\n",
       "         4.32054140e-02, -1.74997933e-02,  4.62776162e-02,\n",
       "         4.59722020e-02, -2.54346859e-02,  1.53148435e-02,\n",
       "         2.12891959e-02,  3.16577591e-02, -4.33956385e-02,\n",
       "        -2.44326238e-02, -4.86536734e-02,  4.44874875e-02,\n",
       "         5.99032640e-03,  3.76216434e-02,  4.94697802e-02,\n",
       "         9.03779268e-03],\n",
       "       [-4.02680263e-02,  2.20146663e-02,  4.56526615e-02,\n",
       "        -1.30012259e-02, -4.75403331e-02, -4.03960347e-02,\n",
       "         3.58297117e-02, -2.91442871e-02, -1.83038227e-02,\n",
       "         3.12304385e-02, -4.92032878e-02,  1.41681321e-02,\n",
       "        -2.49571688e-02, -4.88086231e-02,  3.42076309e-02,\n",
       "         2.58722156e-03,  2.78971530e-02,  1.03770494e-02,\n",
       "        -8.62205029e-03, -3.26845646e-02, -1.92169305e-02,\n",
       "        -1.73451900e-02, -7.47746229e-03,  9.60636884e-04,\n",
       "        -1.83415636e-02,  1.88298263e-02,  3.79093923e-02,\n",
       "        -4.45177928e-02, -1.17330439e-02,  6.94089010e-03,\n",
       "         6.44318014e-03, -3.09315920e-02, -2.88236625e-02,\n",
       "        -2.13404428e-02,  3.02764513e-02,  1.66467316e-02,\n",
       "         1.16816051e-02, -1.84670836e-03,  1.88003294e-02,\n",
       "        -1.43400803e-02,  4.59097289e-02,  1.14045851e-02,\n",
       "         3.74269001e-02, -2.61930376e-03, -4.05906066e-02,\n",
       "        -4.22591455e-02, -4.09877896e-02, -4.22482863e-02,\n",
       "         4.93089221e-02,  2.87833251e-02,  2.83301361e-02,\n",
       "         5.53481653e-03,  1.13811120e-02,  8.61406326e-03,\n",
       "         9.72335413e-03, -3.83052714e-02, -3.06770951e-03,\n",
       "         2.36654393e-02, -1.31729245e-02, -8.71593878e-03,\n",
       "        -1.17382780e-02,  3.31064351e-02, -4.00155187e-02,\n",
       "        -4.20339331e-02],\n",
       "       [ 1.57734416e-02, -4.31489199e-04,  2.62135305e-02,\n",
       "         1.34466924e-02,  3.62539291e-03, -3.44996527e-03,\n",
       "         2.35560648e-02, -4.52846996e-02,  1.52126700e-03,\n",
       "         3.38290669e-02,  3.81874479e-02,  4.91781868e-02,\n",
       "        -5.62071800e-03, -2.45662779e-03,  2.14719772e-03,\n",
       "         4.80485596e-02, -1.13283508e-02,  1.68971457e-02,\n",
       "         3.80861647e-02,  4.67308648e-02,  4.69148643e-02,\n",
       "        -2.77457479e-02,  2.26385854e-02,  3.43989991e-02,\n",
       "        -1.55218355e-02,  2.15463676e-02, -1.05702393e-02,\n",
       "         5.06908819e-03,  1.67436637e-02, -3.12069897e-02,\n",
       "        -3.94579545e-02, -9.95857641e-03,  4.06798162e-02,\n",
       "         8.32378864e-03,  4.29118015e-02, -4.52873223e-02,\n",
       "         1.45047642e-02, -1.72490589e-02,  3.75586636e-02,\n",
       "         2.03009583e-02,  5.16005605e-03,  1.85481794e-02,\n",
       "         6.44940138e-03, -4.74102870e-02, -9.46023315e-03,\n",
       "        -1.38532519e-02,  4.06672992e-02,  4.27150391e-02,\n",
       "        -1.73895359e-02, -1.83653012e-02, -4.28781509e-02,\n",
       "        -7.67321512e-03, -4.63535637e-03,  5.64949587e-03,\n",
       "        -1.25691183e-02, -6.82226568e-03, -3.32703479e-02,\n",
       "         4.37609591e-02,  2.17757858e-02, -4.89524491e-02,\n",
       "        -3.65325212e-02,  4.55710925e-02, -4.17048559e-02,\n",
       "         3.78785282e-03]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# 字彙表最大為1000，輸出維度為 64，輸入的字數為 10\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "# 產生亂數資料，32筆資料，每筆 10 個數字\n",
    "input_array = np.random.randint(1000, size=(32, 10))\n",
    "\n",
    "# 指定損失函數\n",
    "model.compile('rmsprop', 'mse')\n",
    "\n",
    "# 預測\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array.shape)\n",
    "output_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用真實的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000019DBEE35E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 4, 64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 測試資料\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "\n",
    "\n",
    "vocab_size = 50\n",
    "maxlen = 4\n",
    "\n",
    "# 先轉成 one-hot encoding\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "\n",
    "# 轉成固定長度，長度不足則後面補空白\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=maxlen, padding='post')\n",
    "\n",
    "# 模型只有 Embedding\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(vocab_size, 64, input_length=maxlen))\n",
    "model.compile('rmsprop', 'mse')\n",
    "\n",
    "# 預測\n",
    "output_array = model.predict(padded_docs)\n",
    "output_array.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加上Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 80.000001\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "vocab_size = 50\n",
    "maxlen = 4\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=maxlen, padding='post')\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(vocab_size, 8, input_length=maxlen))\n",
    "model.add(layers.Flatten())\n",
    "# 加上一般的完全連接層(Dense)\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加上 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 128)               17536     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 18,065\n",
      "Trainable params: 18,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "vocab_size = 50\n",
    "maxlen = 4\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=maxlen, padding='post')\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(vocab_size, 8, input_length=maxlen))\n",
    "# Add a RNN layer with 128 internal units.\n",
    "model.add(layers.SimpleRNN(128))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用詞向量(Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取 GloVe 100維的詞向量，產生字典資料型的變數，方便搜尋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('./glove/glove.6B.100d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = np.array(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "maxlen = 4\n",
    "\n",
    "\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 轉換為GloVe 100維的詞向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding 設為不需訓練，直接輸入轉換後的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 4, 100)            5000      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 122,377\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 5,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x0000019DC9B78A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# trainable=False\n",
    "model.add(layers.Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
    "\n",
    "# Add a LSTM layer with 128 internal units.\n",
    "model.add(layers.LSTM(128))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
