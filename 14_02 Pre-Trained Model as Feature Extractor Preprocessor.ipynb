{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_KSvMNkCSRhK",
    "outputId": "91d9f843-a096-47bc-e269-960db21afe47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#-*- coding: cp950 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive',force_remount = True)\n",
    "base_dir = '/content/gdrive/MyDrive'\n",
    "path = Path(base_dir +'/Keras_tutorial')  #imgs\n",
    "# path.mkdir(parents=True,exist_ok=True)\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uiYLSccSPS4"
   },
   "outputs": [],
   "source": [
    "#Pre-Trained Model as Classifier\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "# load an image from file\n",
    "image = load_img('./images/elephant.jpg', target_size=(224, 224))\n",
    "\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "\n",
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)\n",
    "\n",
    "# load the model\n",
    "model = VGG16()\n",
    "\n",
    "# predict the probability across all output classes\n",
    "yhat = model.predict(image)\n",
    "\n",
    "# convert the probabilities to class labels\n",
    "label = decode_predictions(yhat)\n",
    "\n",
    "# retrieve the most likely result, e.g. highest probability\n",
    "label = label[0][0]\n",
    "\n",
    "# print the classification\n",
    "print('%s (%.2f%%)' % (label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>Pre-Trained Model as Feature Extractor Preprocessor\n",
    "The pre-trained model may be used as a standalone program to extract features from new photographs.\n",
    "\n",
    "Specifically, the extracted features of a photograph may be a vector of numbers \n",
    "that the model will use to describe the specific features in a photograph. \n",
    "These features can then be used as input in the development of a new model.\n",
    "\n",
    "The last few layers of the VGG16 model are fully connected layers prior to the output layer.\n",
    "These layers will provide a complex set of features to describe a given input image \n",
    "and may provide useful input when training a new model for image classification or related computer vision task.\n",
    "\n",
    "The image can be loaded and prepared for the model, as we did before in the previous example.\n",
    "We will load the model with the classifier output part of the model, \n",
    "but manually remove the final output layer. \n",
    "This means that the second last fully connected layer with 4,096 nodes will be the new output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of using the vgg16 model as a feature extraction model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from pickle import dump\n",
    "\n",
    "# load an image from file\n",
    "image = load_img('./images/elephant.jpg', target_size=(224, 224))\n",
    "\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "\n",
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)\n",
    "\n",
    "# load model\n",
    "model = VGG16()\n",
    "\n",
    "# remove the output layer\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\n",
    "# get extracted features\n",
    "features = model.predict(image)\n",
    "print(features.shape)\n",
    "\n",
    "# save to file\n",
    "dump(features, open('./images/elephant.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Running the example loads the photograph, then prepares the model as a feature extraction model.\n",
    "\n",
    "The features are extracted from the loaded photo and the shape of the feature vector is printed, \n",
    "showing it has 4,096 numbers. This feature is then saved to a new file dog.pkl in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    ">>>Pre-Trained Model as Feature Extractor in Model\n",
    "We can use some or all of the layers in a pre-trained model as a feature extraction component of a new model directly.\n",
    "This can be achieved by loading the model, \n",
    "then simply adding new layers. This may involve adding new convolutional and pooling layers to expand upon the feature extraction capabilities of the model or adding new fully connected classifier type layers to learn how to interpret the extracted features on a new dataset, or some combination.\n",
    "For example, we can load the VGG16 models without the classifier part of the model \n",
    "by specifying the “include_top” argument to “False”, \n",
    "and specify the preferred shape of the images in our new dataset as 300×300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can then use the Keras function API to add a new Flatten layer after the last pooling layer in the VGG16 model,\n",
    "then define a new classifier model with a Dense fully connected layer \n",
    "and an output layer that will predict the probability for 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "An alternative approach to adding a Flatten layer would be to define the VGG16 model with an average pooling layer,\n",
    "and then add fully connected layers. \n",
    "Perhaps try both approaches on your application and see which results in the best performance.\n",
    "\n",
    "The weights of the VGG16 model and the weights for the new model will all be trained together on the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tending the vgg16 model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "# load model without classifier layers\n",
    "model = VGG16(include_top=False, input_shape=(300, 300, 3))\n",
    "\n",
    "# add new classifier layers\n",
    "flat1 = Flatten()(model.layers[-1].output)\n",
    "class1 = Dense(1024, activation='relu')(flat1)\n",
    "output = Dense(10, activation='softmax')(class1)\n",
    "\n",
    "# define new model\n",
    "model = Model(inputs=model.inputs, outputs=output)\n",
    "\n",
    "# summarize\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can see that we have flattened the output of the last pooling layer and added our new fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alternately, we may wish to use the VGG16 model layers, \n",
    "but train the new layers of the model without updating the weights of the VGG16 layers. \n",
    "This will allow the new output layers to learn to interpret the learned features of the VGG16 model.\n",
    "\n",
    "This can be achieved by setting the “trainable” property on each of the layers in the loaded VGG model to False prior to training.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model without classifier layers\n",
    "model = VGG16(include_top=False, input_shape=(300, 300, 3))\n",
    "\n",
    "# mark loaded layers as not trainable\n",
    "for layer in model.layers:\n",
    "layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You can pick and choose which layers are trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For example, perhaps you want to retrain some of the convolutional layers deep in the model, \n",
    "but none of the layers earlier in the model. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model without classifier layers\n",
    "model = VGG16(include_top=False, input_shape=(300, 300, 3))\n",
    "# mark some layers as not trainable\n",
    "model.get_layer('block1_conv1').trainable = False\n",
    "model.get_layer('block1_conv2').trainable = False\n",
    "model.get_layer('block2_conv1').trainable = False\n",
    "model.get_layer('block2_conv2').trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "14_02_Keras_applications2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
