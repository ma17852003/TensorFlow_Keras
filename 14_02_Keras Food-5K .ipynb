{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxzFM5BKSPSu"
   },
   "source": [
    "# 部分採用，擷取特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_KSvMNkCSRhK",
    "outputId": "91d9f843-a096-47bc-e269-960db21afe47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#-*- coding: cp950 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive',force_remount = True)\n",
    "base_dir = '/content/gdrive/MyDrive'\n",
    "path = Path(base_dir +'/Keras_tutorial')  #imgs\n",
    "# path.mkdir(parents=True,exist_ok=True)\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>特徵萃取\n",
    "特徵萃取（feature extraction）的原理，就是將原來模型FC層(全連結層)的分類器移除然後用新的分類器取代，而負責萃取資料的CNN Base則維持不變。\n",
    "\n",
    ">>特徵萃取有兩種作法：無論是第一或第二種方法，皆需要將後端FC層分類器移除\n",
    "1.將特徵存取(CNN Base)與分類器層視為不同的個體，\n",
    "先將dataset的圖片在CNN Base跑過一次，取得所有的特徵向量後，再將這些特徵向量送到分類器進行分類訓練，\n",
    "因此，CNN base僅負責初次的特徵萃取，之後便在分類層上進行訓練\n",
    "此種方式的優點是整體的訓練時間短，因為實際訓練的僅在FC層，而CNN 層在取出特徵圖之後便不再使用，是故所有圖片在CNN層上只需run過一次，\n",
    "後續在FC層的分類器訓練則視epoch次數而定，由於訓練時間很短，因此很適合在CPU或無GPU的環境訓練。\n",
    "不過，此法的缺點則是無法透過ImageDataGenerator on-fly的產生資料強化用的圖片（除非我們事先產生好）。\n",
    "\n",
    "2.將新的FC層分類器附加在已去除分類器的CNN layers後方，\n",
    "形成跟之前一樣完整的模型來進行訓練，跟之前模型差別只在於分類器的不同。\n",
    "此法的好處是可以把它當成一般模型來使用，因此能夠on-fly的透過ImageDataGenerator來產生大量圖片來訓練，\n",
    "缺點是就跟一般的CNN訓練一樣，圖片在CNN layers與FC layers持續的by batch訓練，相當的耗時，因此不適合在無GPU的環境下訓練。\n",
    "\n",
    ">>下方以Keras所內建的VGG16模型為例，示範如何將用於softmax分類的FC layer移除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "If we were to stop propagation before the fully-connected layers in VGG16, \n",
    "the last layer in the network would become the max-pooling layer (Figure 2, right),\n",
    "which will have an output shape of 7 x 7 x 512. Flattening, \n",
    "this volume into a feature vector we would obtain a list of 7 x 7 x 512 = 25,088 values —\n",
    "this list of numbers serves as our feature vector used to quantify the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>Extracting features using Keras and pre-trained CNNs\n",
    "With weights dialed in and by loading our model without the head, we are now ready for transfer learning.\n",
    "We will use the output values of the network directly, storing the results as feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration file\n",
    "# import the necessary packages\n",
    "import os\n",
    "\n",
    "# initialize the path to the *original* input directory of images\n",
    "ORIG_INPUT_DATASET = \"Food-5K\"\n",
    "\n",
    "# initialize the base path to the *new* directory that will contain our images after computing the training and testing split\n",
    "BASE_PATH = \"dataset\"\n",
    "\n",
    "# define the names of the training, testing, and validation directories\n",
    "TRAIN = \"training\"\n",
    "TEST = \"evaluation\"\n",
    "VAL = \"validation\"\n",
    "\n",
    "# initialize the list of class label names\n",
    "CLASSES = [\"non_food\", \"food\"]\n",
    "\n",
    "# set the batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "# we can build the rest of our paths:\n",
    "# initialize the label encoder file path and the output directory to\n",
    "# where the extracted features (in CSV file format) will be stored\n",
    "LE_PATH = os.path.sep.join([\"output\", \"le.cpickle\"])\n",
    "BASE_CSV_PATH = \"output\"\n",
    "\n",
    "# set the path to the serialized model after training\n",
    "MODEL_PATH = os.path.sep.join([\"output\", \"model.cpickle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building our dataset for feature extraction\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from pyimagesearch import config\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s loop over our data splits:\n",
    "# loop over the data splits\n",
    "for split in (config.TRAIN, config.TEST, config.VAL):\n",
    "\t# grab all image paths in the current split\n",
    "\tprint(\"[INFO] processing '{} split'...\".format(split))\n",
    "\tp = os.path.sep.join([config.BASE_PATH, split])\n",
    "\timagePaths = list(paths.list_images(p))\n",
    "\n",
    "\t# randomly shuffle the image paths and then extract the class\n",
    "\t# labels from the file paths\n",
    "\trandom.shuffle(imagePaths)\n",
    "\tlabels = [p.split(os.path.sep)[-2] for p in imagePaths]\n",
    "\n",
    "\t# if the label encoder is None, create it\n",
    "\tif le is None:\n",
    "\t\tle = LabelEncoder()\n",
    "\t\tle.fit(labels)\n",
    "\n",
    "\t# open the output CSV file for writing\n",
    "\tcsvPath = os.path.sep.join([config.BASE_CSV_PATH,\n",
    "\t\t\"{}.csv\".format(split)])\n",
    "\tcsv = open(csvPath, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next step is to loop over our imagePaths in BATCH_SIZE chunks:\n",
    "\t# loop over the images in batches\n",
    "\tfor (b, i) in enumerate(range(0, len(imagePaths), config.BATCH_SIZE)):\n",
    "\t\t# extract the batch of images and labels, then initialize the\n",
    "\t\t# list of actual images that will be passed through the network\n",
    "\t\t# for feature extraction\n",
    "\t\tprint(\"[INFO] processing batch {}/{}\".format(b + 1,\n",
    "\t\t\tint(np.ceil(len(imagePaths) / float(config.BATCH_SIZE)))))\n",
    "\t\tbatchPaths = imagePaths[i:i + config.BATCH_SIZE]\n",
    "\t\tbatchLabels = le.transform(labels[i:i + config.BATCH_SIZE])\n",
    "\t\tbatchImages = []\n",
    "        \n",
    "#Let’s go ahead and populate our batchImages now:\n",
    "\t\t# loop over the images and labels in the current batch\n",
    "\t\tfor imagePath in batchPaths:\n",
    "\t\t\t# load the input image using the Keras helper utility\n",
    "\t\t\t# while ensuring the image is resized to 224x224 pixels\n",
    "\t\t\timage = load_img(imagePath, target_size=(224, 224))\n",
    "\t\t\timage = img_to_array(image)\n",
    "\n",
    "\t\t\t# preprocess the image by (1) expanding the dimensions and\n",
    "\t\t\t# (2) subtracting the mean RGB pixel intensity from the\n",
    "\t\t\t# ImageNet dataset\n",
    "\t\t\timage = np.expand_dims(image, axis=0)\n",
    "\t\t\timage = preprocess_input(image)\n",
    "\n",
    "\t\t\t# add the image to the batch\n",
    "\t\t\tbatchImages.append(image)\n",
    "\n",
    "        \n",
    "#Now we will pass the batch of images through our network to extract features:\n",
    "\t\t# pass the images through the network and use the outputs as\n",
    "\t\t# our actual features, then reshape the features into a\n",
    "\t\t# flattened volume\n",
    "\t\tbatchImages = np.vstack(batchImages)\n",
    "\t\tfeatures = model.predict(batchImages, batch_size=config.BATCH_SIZE)\n",
    "\t\tfeatures = features.reshape((features.shape[0], 7 * 7 * 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Our batch of images is sent through the network via Lines 71 and 72. \n",
    "Keep in mind that we have removed the fully-connected layer head of the network. \n",
    "Instead, the forward propagation stops at the max-pooling layer. \n",
    "We will treat the output of the max-pooling layer as a list of features , also known as a “feature vector”.\n",
    "\n",
    "The output dimension of the max-pooling layer is (batch_size, 7 x 7 x 512). \n",
    "We can thus reshape the features into a NumPy array of shape (batch_size, 7 * 7 * 512),\n",
    "treating the output of the CNN as a feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s wrap up this script:\n",
    "\t\t# loop over the class labels and extracted features\n",
    "\t\tfor (label, vec) in zip(batchLabels, features):\n",
    "\t\t\t# construct a row that exists of the class label and\n",
    "\t\t\t# extracted features\n",
    "\t\t\tvec = \",\".join([str(v) for v in vec])\n",
    "\t\t\tcsv.write(\"{},{}\\n\".format(label, vec))\n",
    "\n",
    "\t# close the CSV file\n",
    "\tcsv.close()\n",
    "\n",
    "# serialize the label encoder to disk\n",
    "f = open(config.LE_PATH, \"wb\")\n",
    "f.write(pickle.dumps(le))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Maintaining our batch efficiency, the features and associated class labels are written to our CSV file (Lines 76-80).\n",
    "\n",
    "Inside the CSV file, the class label is the first field in each row (enabling us to easily extract it from the row during training). The feature vec follows.\n",
    "Each CSV file will be closed via Line 83. Recall that upon completion we will have one CSV file per data split.\n",
    "Finally, we can dump the label encoder to disk (Lines 86-88)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jga9GgsMTMUy",
    "outputId": "28a58f34-a11f-468a-fd04-c5c6f6076816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can then repeat the process for our entire dataset of images.\n",
    "\n",
    "Given a total of N images in our network, our dataset would now be represented as a list of N vectors, each of 25,088-dim.\n",
    "Once we have our feature vectors, we can train off-the-shelf machine learning models \n",
    "such as Linear SVM, Logistic Regression, Decision Trees, or Random Forests on top of these features to obtain a classifier \n",
    "that can recognize new classes of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "That said, the two most common machine learning models you’ll see for transfer learning via feature extraction are:\n",
    "1Logistic Regression\n",
    "2Linear SVM\n",
    "\n",
    "Why those two models?\n",
    "First, keep in mind our feature extractor is a CNN.\n",
    "CNN’s are non-linear models capable of learning non-linear features — \n",
    "we are assuming that the features learned by the CNN are already robust and discriminative.\n",
    "\n",
    "The second, and perhaps arguably more important reason, \n",
    "is that our feature vectors tend to be very large and have high dimensionality.\n",
    "We, therefore, need a fast model that can be trained on top of the features — linear models tend to be very fast to train.\n",
    "For example, our dataset of 5,000 images, each represented by a feature vector of 25,088-dim,\n",
    "can be trained in a few seconds using a Logistic Regression model.\n",
    "\n",
    "To wrap up this section, I want you to keep in mind that the CNN itself is not capable of recognizing these new classes.\n",
    "Instead, we are using the CNN as an intermediary feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>Implementing our training script\n",
    "The final step for transfer learning via feature extraction is to implement a Python script \n",
    "that will take our extracted features from the CNN and then train a Logistic Regression model on top of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from pyimagesearch import config\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def load_data_split(splitPath):\n",
    "\t# initialize the data and labels\n",
    "\tdata = []\n",
    "\tlabels = []\n",
    "\n",
    "\t# loop over the rows in the data split file\n",
    "\tfor row in open(splitPath):\n",
    "\t\t# extract the class label and features from the row\n",
    "\t\trow = row.strip().split(\",\")\n",
    "\t\tlabel = row[0]\n",
    "\t\tfeatures = np.array(row[1:], dtype=\"float\")\n",
    "\n",
    "\t\t# update the data and label lists\n",
    "\t\tdata.append(features)\n",
    "\t\tlabels.append(label)\n",
    "\n",
    "\t# convert the data and labels to NumPy arrays\n",
    "\tdata = np.array(data)\n",
    "\tlabels = np.array(labels)\n",
    "\n",
    "\t# return a tuple of the data and labels\n",
    "\treturn (data, labels)\n",
    "\n",
    "#With the load_data_spit function ready to go, let’s put it to work by loading our data:\n",
    "# derive the paths to the training and testing CSV files\n",
    "trainingPath = os.path.sep.join([config.BASE_CSV_PATH,\n",
    "\t\"{}.csv\".format(config.TRAIN)])\n",
    "testingPath = os.path.sep.join([config.BASE_CSV_PATH,\n",
    "\t\"{}.csv\".format(config.TEST)])\n",
    "\n",
    "# load the data from disk\n",
    "print(\"[INFO] loading data...\")\n",
    "(trainX, trainY) = load_data_split(trainingPath)\n",
    "(testX, testY) = load_data_split(testingPath)\n",
    "\n",
    "# load the label encoder from disk\n",
    "le = pickle.loads(open(config.LE_PATH, \"rb\").read())\n",
    "\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "model = LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\",\n",
    "\tmax_iter=150)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# evaluate the model\n",
    "print(\"[INFO] evaluating...\")\n",
    "preds = model.predict(testX)\n",
    "print(classification_report(testY, preds, target_names=le.classes_))\n",
    "\n",
    "# serialize the model to disk\n",
    "print(\"[INFO] saving model...\")\n",
    "f = open(config.MODEL_PATH, \"wb\")\n",
    "f.write(pickle.dumps(model))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uiYLSccSPS4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "14_02_Keras_applications2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
